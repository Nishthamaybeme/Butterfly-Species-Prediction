{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nishthamaybeme/Butterfly-Species-Prediction/blob/main/vit_butterfly.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cnG4oqltNgr",
        "outputId": "d6b3b4f4-8480-4252-ea73-a51a7ac17384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw1A6sFHtc5S",
        "outputId": "4aa83d64-c16a-4df0-b10c-262abb580f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset contains 832 samples.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "class LeedsButterflyDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.image_dir = os.path.join(root_dir, 'images')\n",
        "        self.image_files = [f for f in os.listdir(self.image_dir) if f.endswith('.png')]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        # Fix the label extraction: get first 3 digits as category ID\n",
        "        category_id = int(self.image_files[idx][:3])  # Extract the category ID (first 3 digits)\n",
        "        label = category_id - 1  # Category IDs start from 1, but labels start from 0\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Set dataset path\n",
        "DATASET_PATH = '/content/drive/MyDrive/crime game/leedsbutterfly'\n",
        "\n",
        "# Define transformation (resize to 224x224 for ViT input)\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images for ViT\n",
        "    transforms.ToTensor(),          # Convert to PyTorch Tensor\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize\n",
        "])\n",
        "\n",
        "# Load dataset\n",
        "dataset = LeedsButterflyDataset(DATASET_PATH, transform=transform)\n",
        "\n",
        "# Create data loader\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Check if dataset is loaded correctly\n",
        "print(f\"Dataset contains {len(dataset)} samples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wp1gjeXuA-o",
        "outputId": "f1196ff8-88b9-4c2e-d2fb-0ba27775c1d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import ViTForImageClassification, ViTFeatureExtractor\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the pre-trained ViT model and feature extractor\n",
        "model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=10)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Load the feature extractor for ViT\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ml6249lgXx7",
        "outputId": "049cce0c-4a5f-4fcf-f389-4f40ef934515"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.4790197610855103\n",
            "Epoch [2/10], Loss: 0.4723818382391563\n",
            "Epoch [3/10], Loss: 0.23945709088673958\n",
            "Epoch [4/10], Loss: 0.16730924982291\n",
            "Epoch [5/10], Loss: 0.13031962886452675\n",
            "Epoch [6/10], Loss: 0.1058353830415469\n",
            "Epoch [7/10], Loss: 0.08791764195148762\n",
            "Epoch [8/10], Loss: 0.0742521326129253\n",
            "Epoch [9/10], Loss: 0.06351242644282487\n",
            "Epoch [10/10], Loss: 0.054906462104274675\n"
          ]
        }
      ],
      "source": [
        "# Training loop\n",
        "epochs = 10  # Number of epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images).logits  # Logits are the raw predictions\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {running_loss / len(train_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwgO97TpuJVK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the directory path\n",
        "save_directory = '/content/drive/My Drive/new_directory'\n",
        "\n",
        "# Create the directory if it does not exist\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# Define the full save path for the model\n",
        "save_path = os.path.join(save_directory, 'vit_butterfly_model1.pth')\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), save_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-xn6y8Nozdon",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d74bf8c-6162-497b-ae3a-52e08f409a3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "<ipython-input-8-1753fc30e13c>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('//content/drive/MyDrive/new_directory/vit_butterfly_model1.pth'))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ViTForImageClassification(\n",
              "  (vit): ViTModel(\n",
              "    (embeddings): ViTEmbeddings(\n",
              "      (patch_embeddings): ViTPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (dropout): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): ViTEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x ViTLayer(\n",
              "          (attention): ViTSdpaAttention(\n",
              "            (attention): ViTSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): ViTSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): ViTIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): ViTOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Re-initialize the model (same architecture as before)\n",
        "model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224-in21k', num_labels=10)\n",
        "\n",
        "# Load the saved weights\n",
        "model.load_state_dict(torch.load('//content/drive/MyDrive/new_directory/vit_butterfly_model1.pth'))\n",
        "\n",
        "# Move the model to the device (GPU or CPU)\n",
        "model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFxDrvBWzkZW",
        "outputId": "2012aeb3-17c0-4804-afaa-aa5eda312264"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: 0\n"
          ]
        }
      ],
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "\n",
        "# Define the image preprocessing pipeline\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),   # Resize to the input size of ViT\n",
        "    transforms.ToTensor(),           # Convert image to tensor\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize to the ImageNet mean and std\n",
        "])\n",
        "\n",
        "# Load a new image\n",
        "image_path = '/content/drive/MyDrive/images.jpg'  # Replace with your image path\n",
        "image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "# Apply the preprocessing transform\n",
        "image = transform(image).unsqueeze(0)  # Add a batch dimension\n",
        "\n",
        "# Move the image to the device\n",
        "image = image.to(device)\n",
        "\n",
        "# Make the prediction\n",
        "with torch.no_grad():\n",
        "    outputs = model(image).logits\n",
        "    _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "# Output the predicted class label\n",
        "print(f\"Predicted class: {predicted.item()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiQih4-Shz0-",
        "outputId": "45f2d8ab-0c68-47d9-eddb-25fd909e60c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted butterfly species:\n",
            "Scientific Name: Danaus plexippus\n",
            "Common Name: Monarch Butterfly\n"
          ]
        }
      ],
      "source": [
        "# Map class indices to butterfly species (scientific and common names)\n",
        "class_names = {\n",
        "    0: ('Danaus plexippus', 'Monarch Butterfly'),\n",
        "    1: ('Heliconius charitonius', 'Zebra Longwing'),\n",
        "    2: ('Heliconius erato', 'Red Postman'),\n",
        "    3: ('Junonia coenia', 'Common Buckeye'),\n",
        "    4: ('Lycaena phlaeas', 'Small Copper'),\n",
        "    5: ('Nymphalis antiopa', 'Mourning Cloak'),\n",
        "    6: ('Papilio cresphontes', 'Giant Swallowtail'),\n",
        "    7: ('Pieris rapae', 'Cabbage White'),\n",
        "    8: ('Vanessa atalanta', 'Red Admiral'),\n",
        "    9: ('Vanessa cardui', 'Painted Lady')\n",
        "}\n",
        "\n",
        "# Get the predicted class index (for example, class index 3)\n",
        "predicted_class_idx = predicted.item()\n",
        "\n",
        "# Get the scientific and common names for the predicted class\n",
        "predicted_scientific_name, predicted_common_name = class_names[predicted_class_idx]\n",
        "\n",
        "# Print both names\n",
        "print(f\"Predicted butterfly species:\")\n",
        "print(f\"Scientific Name: {predicted_scientific_name}\")\n",
        "print(f\"Common Name: {predicted_common_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXLqWVH6h26a",
        "outputId": "b65117f9-9b0e-407e-b709-71952683cab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_true' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-a1194dba45d7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Calculate Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Accuracy: {accuracy:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y_true' is not defined"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming you have the true labels (y_true) and predicted labels (y_pred)\n",
        "# Example:\n",
        "# y_true = [true species labels]\n",
        "# y_pred = [predicted species labels from your model]\n",
        "\n",
        "# Calculate Accuracy\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Classification report (includes precision, recall, and F1-score for each class)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true, y_pred))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=species_classes, yticklabels=species_classes)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p73WoAqgKtin"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNO6r/mEAVoEz2RHV03JFx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}